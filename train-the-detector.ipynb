{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --user tflearn\n",
    "#!pip install --user librosa 'llvmlite==0.19.0'\n",
    "#!pip install --user samplerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3500, 7, dict_keys(['1', '2', '3', '4', '5', '6', 'sil']), (16, 40))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.io import wavfile\n",
    "from tflearn.data_utils import to_categorical\n",
    "import numpy as np\n",
    "import random\n",
    "import librosa\n",
    "import glob\n",
    "import samplerate\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "dataset = []\n",
    "label_to_idx = dict()\n",
    "idx_to_label = dict()\n",
    "time_size = 16\n",
    "feat_size = 40\n",
    "\n",
    "rate, noise_data = wavfile.read('dataset/noise.wav')\n",
    "noise_rest = len(noise_data) - rate\n",
    "\n",
    "def normalize(data):\n",
    "    data = data.astype(float)\n",
    "    mn, mx = data.min(), data.max()\n",
    "    data = (data - mn) / (mx - mn) * 65535 - 32768\n",
    "    return data.astype('short')\n",
    "\n",
    "def read_dataset():\n",
    "    for fname in glob.glob('dataset/*/*.wav', recursive=True):\n",
    "        label = fname.split('.')[0].split('/')[1:]\n",
    "        rate, data = wavfile.read(fname)\n",
    "        data = normalize(data)\n",
    "        yield label, data\n",
    "\n",
    "def augment_resample(dataset):\n",
    "    for info, data in dataset:\n",
    "        for rs in (1, 0.75, 1.15, 0.85, 0.9):\n",
    "            rsdata = samplerate.resample(data, rs, 'sinc_best').astype('short')\n",
    "            yield (*info, rs), rsdata\n",
    "\n",
    "def augment_pad(dataset):\n",
    "    for info, data in dataset:\n",
    "        for _ in range(5):\n",
    "            file_rest = rate - len(data)\n",
    "            if file_rest <= 0:\n",
    "                file_data = data[:rate]\n",
    "            else:\n",
    "                file_offset = int(random.uniform(0, file_rest))\n",
    "                file_data = np.pad(data, (file_offset, file_rest - file_offset), 'constant', constant_values=(0,0))\n",
    "            yield info, file_data\n",
    "\n",
    "def augment_noise(dataset):\n",
    "    for info, data in dataset:\n",
    "        for noise_level in range(5):\n",
    "            noise_offset = int(random.uniform(0, noise_rest))\n",
    "            noise_sample = noise_data[noise_offset:noise_offset + rate] * noise_level\n",
    "            file_data = data.astype(float) + noise_sample.astype(float)\n",
    "            file_data = normalize(file_data)\n",
    "            yield (*info, noise_level), file_data\n",
    "\n",
    "for info, data in augment_noise(augment_pad(augment_resample(read_dataset()))):\n",
    "#     wavfile.write('temp/%s.wav' % (info,), rate, data)\n",
    "\n",
    "    mfcc = np.transpose(librosa.feature.mfcc(data, rate, n_mfcc=feat_size))\n",
    "\n",
    "    label = info[0]\n",
    "    idx = label_to_idx.get(label, None)\n",
    "    if idx is None:\n",
    "        idx = len(label_to_idx)\n",
    "        label_to_idx[label] = idx\n",
    "        idx_to_label[idx] = label\n",
    "    dataset.append((mfcc, idx, info))\n",
    "random.shuffle(dataset)\n",
    "\n",
    "inp_data = [r[0] for r in dataset]\n",
    "out_size = len(label_to_idx)\n",
    "out_data = to_categorical([r[1] for r in dataset], out_size)\n",
    "\n",
    "len(inp_data), out_size, label_to_idx.keys(), inp_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1100  | total loss: \u001b[1m\u001b[32m0.02805\u001b[0m\u001b[0m | time: 7.321s\n",
      "\u001b[2K\r",
      "| Adam | epoch: 020 | loss: 0.02805 - acc: 0.9924 -- iter: 3480/3480\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tflearn\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "check_size = 20 #int(len(inp_data) * 0.1)\n",
    "learn_size = len(inp_data) - check_size\n",
    "trainX = inp_data[:learn_size]\n",
    "trainY = out_data[:learn_size]\n",
    "testX = inp_data[-check_size:]\n",
    "testY = out_data[-check_size:]\n",
    "\n",
    "g = tflearn.input_data(shape=[None,time_size,feat_size])\n",
    "g = tflearn.reshape(g, [-1,time_size,feat_size,1])\n",
    "\n",
    "g = tflearn.conv_2d(g, 16, (20,8), activation='relu')\n",
    "g = tflearn.dropout(g, 0.8)\n",
    "g = tflearn.max_pool_2d(g, 4)\n",
    "g = tflearn.local_response_normalization(g)\n",
    "g = tflearn.reshape(g, [-1,time_size,feat_size])\n",
    "\n",
    "g = tflearn.gru(g, 384, dropout=0.7)\n",
    "\n",
    "g = tflearn.fully_connected(g, out_size, activation='softmax')\n",
    "g = tflearn.regression(g, optimizer='adam', loss='categorical_crossentropy', learning_rate=0.001)\n",
    "m = tflearn.DNN(g)\n",
    "m.fit(trainX, trainY, n_epoch=20, show_metric=True, snapshot_epoch=False, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ 9% 1: ('1', 'b', 1, 1)\n",
      "+ 9% sil: ('sil', 'g', 0.75, 2)\n",
      "+ 9% 1: ('1', 'a', 0.75, 2)\n",
      "+ 9% 3: ('3', 'a', 0.85, 1)\n",
      "+ 9% 2: ('2', 'a', 1.15, 4)\n",
      "+ 9% sil: ('sil', 'g', 1.15, 3)\n",
      "+ 9% 3: ('3', 'd', 0.85, 0)\n",
      "+ 9% sil: ('sil', 'd', 0.9, 0)\n",
      "+ 9% sil: ('sil', 'e', 0.9, 0)\n",
      "+ 9% 1: ('1', 'a', 1.15, 3)\n",
      "+ 9% sil: ('sil', 'd', 0.75, 3)\n",
      "+ 9% sil: ('sil', 'd', 0.9, 0)\n",
      "+ 9% 4: ('4', 'b', 0.9, 2)\n",
      "+ 9% sil: ('sil', 'g', 1, 0)\n",
      "+ 6% 1: ('1', 'e', 0.75, 3)\n",
      "+ 9% 2: ('2', 'd', 1.15, 3)\n",
      "+ 9% sil: ('sil', 'b', 0.75, 1)\n",
      "+ 9% 2: ('2', 'a', 0.85, 3)\n",
      "+ 9% 1: ('1', 'c', 0.75, 0)\n",
      "+ 9% 3: ('3', 'c', 1, 0)\n",
      "20 0 0 6\n",
      "---\n",
      "v 9 1 ('1', 'c', 0.75, 3)\n",
      "v 9 sil ('sil', 'f', 0.85, 3)\n",
      "v 9 5 ('5', 'b', 1.15, 0)\n",
      "v 9 3 ('3', 'c', 0.75, 1)\n",
      "v 9 3 ('3', 'e', 1.15, 2)\n",
      "v 9 2 ('2', 'd', 1, 1)\n",
      "v 9 sil ('sil', 'e', 0.9, 4)\n",
      "v 9 5 ('5', 'b', 0.85, 2)\n",
      "v 9 3 ('3', 'e', 1.15, 3)\n",
      "v 9 3 ('3', 'a', 0.9, 4)\n",
      "v 9 4 ('4', 'b', 1.15, 3)\n",
      "v 9 4 ('4', 'b', 0.9, 1)\n",
      "v 9 4 ('4', 'a', 0.75, 0)\n",
      "v 9 1 ('1', 'd', 1.15, 3)\n",
      "v 9 5 ('5', 'b', 0.9, 3)\n",
      "v 9 sil ('sil', 'b', 1.15, 2)\n",
      "v 9 3 ('3', 'b', 1.15, 3)\n",
      "v 9 6 ('6', 'a', 0.85, 0)\n",
      "v 9 3 ('3', 'c', 0.75, 3)\n",
      "v 9 4 ('4', 'b', 0.85, 3)\n",
      "v 9 1 ('1', 'a', 0.9, 3)\n",
      "v 9 5 ('5', 'a', 0.85, 0)\n",
      "v 9 2 ('2', 'b', 1, 1)\n",
      "v 9 1 ('1', 'a', 0.9, 0)\n",
      "v 9 sil ('sil', 'g', 1.15, 4)\n",
      "v 9 6 ('6', 'b', 0.85, 2)\n",
      "v 9 sil ('sil', 'c', 1, 4)\n",
      "v 9 5 ('5', 'b', 1, 1)\n",
      "v 9 5 ('5', 'b', 0.9, 1)\n",
      "v 9 sil ('sil', 'b', 1, 3)\n",
      "v 9 3 ('3', 'e', 1.15, 4)\n",
      "v 9 6 ('6', 'a', 0.9, 1)\n",
      "v 9 2 ('2', 'e', 0.75, 4)\n",
      "v 9 3 ('3', 'd', 0.9, 0)\n",
      "v 9 3 ('3', 'b', 0.75, 1)\n",
      "v 9 6 ('6', 'a', 1, 4)\n",
      "v 9 3 ('3', 'a', 1.15, 2)\n",
      "v 9 2 ('2', 'c', 1, 0)\n",
      "v 9 3 ('3', 'a', 0.85, 3)\n",
      "v 9 1 ('1', 'b', 0.85, 0)\n"
     ]
    }
   ],
   "source": [
    "hits, miss = 0, 0\n",
    "mxpe, mnpv = 0, 10\n",
    "for i, out in enumerate(m.predict(testX)):\n",
    "    i0 = out.argsort()[-1]\n",
    "    lbl = idx_to_label[i0]\n",
    "    info = dataset[learn_size + i][2]\n",
    "    p = int(out[i0]*10)\n",
    "    ok = info[0] == lbl\n",
    "    if ok:\n",
    "        hits += 1\n",
    "        mnpv = min(mnpv, p)\n",
    "    else:\n",
    "        miss += 1\n",
    "        mxpe = max(mxpe, p)\n",
    "    print('%s %d%% %s: %s' % (('+' if ok else '-'), p, lbl, info))\n",
    "print(hits, miss, mxpe, mnpv)\n",
    "print('---')\n",
    "for f, _, info in dataset[100:140]:\n",
    "    out = m.predict([f])[0]\n",
    "    i0 = out.argsort()[-1]\n",
    "    lbl = idx_to_label[i0]\n",
    "    print(('v' if info[0] == lbl else '-'), int(out[i0]*10), lbl, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "rate, wave = wavfile.read('dataset/sil/a.wav')\n",
    "rate, wave = wavfile.read('test3.wav')\n",
    "# rate, wave = wavfile.read('approach2/tt.wav')\n",
    "# rate, wave = wavfile.read('../approach2/123.wav') #23s2s21sxs3\n",
    "# rate, wave = wavfile.read('approach2/123456.wav')\n",
    "# rate, wave = wavfile.read('approach2/testn.wav')\n",
    "buffer = wave[:4000]\n",
    "labels = ''\n",
    "for o in range(4000, wave.shape[0], 4000):\n",
    "    chunk = wave[o:o + 4000]\n",
    "    buffer = np.append(buffer, chunk)[-8000:]\n",
    "    norm = normalize(buffer)\n",
    "#     wavfile.write('temp/x-%d.wav' % (o,), rate, norm)\n",
    "    mfcc = np.transpose(librosa.feature.mfcc(norm, rate, n_mfcc=feat_size))\n",
    "    for out in m.predict([mfcc]):\n",
    "        i0 = out.argsort()[-1]\n",
    "        lbl = idx_to_label[i0]\n",
    "        labels += lbl\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:/home/jovyan/anti-swear/model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "m.save('model.tflearn')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
